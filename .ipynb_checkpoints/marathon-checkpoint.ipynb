{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.ticker as mtick\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "from datetime import date\n",
    "import seaborn as sb\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import mstats\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.color_palette(\"Set2\", 10)\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16164 entries, 0 to 16163\n",
      "Data columns (total 21 columns):\n",
      "25k          16164 non-null object\n",
      "age          16164 non-null int64\n",
      "name         16164 non-null object\n",
      "division     16164 non-null int64\n",
      "10k          16164 non-null object\n",
      "gender       16164 non-null object\n",
      "half         16164 non-null object\n",
      "official     16164 non-null float64\n",
      "bib          16164 non-null object\n",
      "ctz          757 non-null object\n",
      "country      16164 non-null object\n",
      "overall      16164 non-null int64\n",
      "pace         16164 non-null float64\n",
      "state        14701 non-null object\n",
      "30k          16164 non-null object\n",
      "5k           16164 non-null object\n",
      "genderdiv    16164 non-null int64\n",
      "20k          16164 non-null object\n",
      "35k          16164 non-null object\n",
      "city         16163 non-null object\n",
      "40k          16164 non-null object\n",
      "dtypes: float64(2), int64(4), object(15)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bm2013 = pd.read_csv('C://Users/ezzan/Downloads/2013.csv')\n",
    "bm2013.head()\n",
    "bm2013.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm2013 = bm2013.drop(['name', 'bib', 'ctz', 'state', 'city', 'country', 'genderdiv'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['25k', 'half', '30k','10k', '20k', '35k', '40k', '5k']\n",
    "bm2013[cols] = bm2013[cols].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "bm2013['gender'] = bm2013.gender.map(lambda x: 0 if x is 'F' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm2013 = bm2013.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26598, 18)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_2013.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 13)) while a minimum of 1 is required by the normalize function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-d03901e450f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;31m# Normalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m \u001b[0mX_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;31m# Data frame to store features and predicted cluster memberships.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[0;32m   1410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m     X = check_array(X, sparse_format, copy=copy,\n\u001b[1;32m-> 1412\u001b[1;33m                     estimator='the normalize function', dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[0;32m   1413\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    460\u001b[0m                              \u001b[1;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[1;32m--> 462\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 13)) while a minimum of 1 is required by the normalize function."
     ]
    }
   ],
   "source": [
    "\n",
    "# Make sure the number of rows divides evenly into four samples.\n",
    "rows = bm2013.shape[0] - bm2013.shape[0] % 4\n",
    "bm2013 = bm2013.iloc[:rows, :]\n",
    "\n",
    "\n",
    "X = bm2013\n",
    "# Normalize\n",
    "X_norm = normalize(X)\n",
    "\n",
    "# Data frame to store features and predicted cluster memberships.\n",
    "ypred = pd.DataFrame()\n",
    "\n",
    "# Create the two-feature PCA for graphing purposes.\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_norm)\n",
    "\n",
    "# Split the data into four equally-sized samples. First we break it in half:\n",
    "X_half1, X_half2, X_pcahalf1, X_pcahalf2 = train_test_split(\n",
    "    X_norm,\n",
    "    X_pca,\n",
    "    test_size=0.5,\n",
    "    random_state=42)\n",
    "\n",
    "# Then we halve the halves.\n",
    "X1, X2, X_pca1, X_pca2 = train_test_split(\n",
    "    X_half1,\n",
    "    X_pcahalf1,\n",
    "    test_size=0.5,\n",
    "    random_state=42)\n",
    "X3, X4, X_pca3, X_pca4 = train_test_split(\n",
    "    X_half2,\n",
    "    X_pcahalf2,\n",
    "    test_size=0.5,\n",
    "    random_state=42)\n",
    "\n",
    "# Pass a list of tuples and a counter that increments each time we go\n",
    "# through the loop. The tuples are the data to be used by k-means,\n",
    "# and the PCA-derived features for graphing. We use k-means to fit a\n",
    "# model to the data, then store the predicted values and the two-feature\n",
    "# PCA solution in the data frame.\n",
    "for counter, data in enumerate([\n",
    "    (X1, X_pca1),\n",
    "    (X2, X_pca2),\n",
    "    (X3, X_pca3),\n",
    "    (X4, X_pca4)]):\n",
    "    \n",
    "    # Put the features into ypred.\n",
    "    ypred['pca_f1' + '_sample' + str(counter)] = data[1][:, 0]\n",
    "    ypred['pca_f2' + '_sample' + str(counter)] = data[1][:, 1]\n",
    "    \n",
    "    # Generate cluster predictions and store them for clusters 2 to 4.\n",
    "    for nclust in range(2, 5):\n",
    "        pred = KMeans(n_clusters=nclust, random_state=42).fit_predict(data[0])\n",
    "        ypred['clust' + str(nclust) + '_sample' + str(counter)] = pred\n",
    "# Break into a set of features and a variable for the known outcome.\n",
    "X = results_2015.iloc[:, :13]\n",
    "y = results_2015.iloc[:, 13]\n",
    "\n",
    "# Replace some random string values.\n",
    "X = X.replace(to_replace='?', value=0)\n",
    "\n",
    "# Binarize y so that 1 means heart disease diagnosis and 0 means no diagnosis.\n",
    "y = np.where(y > 0, 0, 1)\n",
    "\n",
    "# Normalize\n",
    "X_norm = normalize(X)\n",
    "\n",
    "# Data frame to store features and predicted cluster memberships.\n",
    "ypred = pd.DataFrame()\n",
    "\n",
    "# Create the two-feature PCA for graphing purposes.\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_norm)\n",
    "\n",
    "# Split the data into four equally-sized samples. First we break it in half:\n",
    "X_half1, X_half2, X_pcahalf1, X_pcahalf2 = train_test_split(\n",
    "    X_norm,\n",
    "    X_pca,\n",
    "    test_size=0.5,\n",
    "    random_state=42)\n",
    "\n",
    "# Then we halve the halves.\n",
    "X1, X2, X_pca1, X_pca2 = train_test_split(\n",
    "    X_half1,\n",
    "    X_pcahalf1,\n",
    "    test_size=0.5,\n",
    "    random_state=42)\n",
    "X3, X4, X_pca3, X_pca4 = train_test_split(\n",
    "    X_half2,\n",
    "    X_pcahalf2,\n",
    "    test_size=0.5,\n",
    "    random_state=42)\n",
    "\n",
    "# Pass a list of tuples and a counter that increments each time we go\n",
    "# through the loop. The tuples are the data to be used by k-means,\n",
    "# and the PCA-derived features for graphing. We use k-means to fit a\n",
    "# model to the data, then store the predicted values and the two-feature\n",
    "# PCA solution in the data frame.\n",
    "for counter, data in enumerate([\n",
    "    (X1, X_pca1),\n",
    "    (X2, X_pca2),\n",
    "    (X3, X_pca3),\n",
    "    (X4, X_pca4)]):\n",
    "    \n",
    "    # Put the features into ypred.\n",
    "    ypred['pca_f1' + '_sample' + str(counter)] = data[1][:, 0]\n",
    "    ypred['pca_f2' + '_sample' + str(counter)] = data[1][:, 1]\n",
    "    \n",
    "    # Generate cluster predictions and store them for clusters 2 to 4.\n",
    "    for nclust in range(2, 5):\n",
    "        pred = KMeans(n_clusters=nclust, random_state=42).fit_predict(data[0])\n",
    "        ypred['clust' + str(nclust) + '_sample' + str(counter)] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-d869d8e177cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilhouette_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'euclidean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "for sample in [X1, X2, X3, X4]:\n",
    "    model = KMeans(n_clusters=2, random_state=42).fit(sample)\n",
    "    labels = model.labels_\n",
    "    print(metrics.silhouette_score(sample, labels, metric='euclidean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
